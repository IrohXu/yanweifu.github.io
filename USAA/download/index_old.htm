<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0048)http://www.eecs.qmul.ac.uk/~yf300/USAA/download/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script>window["_GOOG_TRANS_EXT_VER"] = "1";</script></head><body><div style="text-align: left;"><br></div><div style="text-align: center;"><h2>Attribute Learning for Understanding Unstructured Social Activity</h2>Yanwei Fu, Timothy M. Hospedales, Tao Xiang, and Shaogang Gong<br>School of EECS, Queen Mary University of London, UK {yanwei.fu,tmh,txiang,sgg}@eecs.qmul.ac.uk<br><br>Abstract<br><div style="text-align: left;">The
rapid development of social video sharing platforms has created a huge
demand for automatic video classification and annotation techniques, in
particular for videos containing social activities of a group of people
(e.g. YouTube video of a wedding reception). Recently, attribute
learning has emerged as a promising paradigm for transferring learning
to sparsely labelled classes in object or single-object short action
classification. In contrast to existing work, this paper for the first
time, tackles the problem of attribute learning for understanding group
social activities with sparse labels. This problem is more challenging
because of the complex multi-object nature of social activities, and
the unstructured nature of the activity context. To solve this problem,
we (1) contribute an unstructured social activity attribute (USAA)
dataset with both visual and audio attributes, (2) introduce the
concept of semi-latent attribute space and (3) propose a novel model
for learning the latent at- tributes which alleviate the dependence of
existing models on exact and exhaustive manual specification of the
attribute-space.We show that our framework is able to exploit latent
attributes to outperform contemporary approaches for addressing a
variety of realistic multi-media sparse data learning tasks including:
multi-task learning, N-shot transfer learn- ing, learning with label
noise and importantly zero-shot learning.<br></div><br><br>
<div style="text-align: left;"><a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/input.mat">Dataset</a>[55M] <a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/eccv2012_poster.pdf"><span style="text-decoration: underline;">Poster</span></a><br>
<a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/Fu%20et%20al_2012_%20Attribute%20Learning%20for%20Understanding%20Unstructured%20Social%20Activity-annotated.pdf">Paper</a><a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/eccv2012_poster.pdf"></a> 
<a href="http://www.eecs.qmul.ac.uk/~yf300/Papers/Published/attr_paper_rev2_v2_bio.pdf">Journal version"Learning Multi-modal Latent Attributes"(TPAMI 2013)</a> </div>
<br>
<div style="text-align: left;">The names of attribute and classes are <a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/attribute_name.html"><span style="text-decoration: underline;">attr</span></a> and <a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/class_name.html">cls</a>.&nbsp; There attributes definitions are referring the video ontoloy definitions in [2].The
source codes are available as soon as our Journal paper are published.
However, we will soon release the matlab P-code first, because we make
many acceleration in our topic model codes.<br>
</div>
<br><br><div style="text-align: left;"><span style="font-weight: bold;">Some Explainations about dataset:</span><br>The dataset mat files includes the following strucutures: <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp;  Xtrain: the low level features of training video data<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; Xtest: the low level features of testing video data.<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;  The low-level features are extracted strictly by [1]. <span style="font-weight: bold;"> The low-level features are extracted
strictly by [1].&nbsp; (Note that there are several videos of all 0
low-level features (&lt;5) due to some problems of extracting process. The low-level features are soft-weighted.)</span><br><br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  train_attr: the user-defined 69 binary video attributes of training videos<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp;  test_attr: the user-defined 69 binary video attributes for testing videos.&nbsp; <br>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; train_video_label: the training video class labels for the 8 classes<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp;  test_video_label: the testing video class labels for the 8 classes<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp;  <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp;  train_video_name: the index of&nbsp; training videos<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; test_video_name: the index of testing videos<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;  &nbsp;&nbsp; train_video_name and test_video_name are the index of the following two <a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/CCVYoutubeIDs.tar.gz">Youtube IDs files.</a>&nbsp;
These two files contains unique YouTube IDs of 4659 training videos and
YouTube IDs of 4658 test videos. And the index are the line number
corresponding to these two files. These Youtube IDs comes from <a href="http://www.ee.columbia.edu/ln/dvmm/CCV/">CCV dataset</a>.
&nbsp; (We hope to release the original video data, but it seems that
there will be some authourity problems from Youtube. So, we can only
provide the Youtube IDs. However, you can easily download the videos by
using the <a href="http://www.eecs.qmul.ac.uk/~yf300/USAA/download/youtube-dl.py">python downloader.</a>My download
method is to generate a Linux batch program(.sh) and each line invokes
the python downloader file&nbsp; downloading one video. )<br>&nbsp;<br><br></div><br><div style="text-align: left;">References:<br></div><div style="text-align: left;">[1] <span style="color: rgb(17, 17, 17); font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; display: inline ! important; float: none;">Yu-Gang Jiang, Jun Yang, Chong-Wah Ngo, Alexander G. Hauptmann,</span><b style="color: rgb(17, 17, 17); font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px;">Representations of Keypoint-Based Semantic Concept Detection: A Comprehensive Study</b><span style="color: rgb(17, 17, 17); font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; display: inline ! important; float: none;"><span class="Apple-converted-space">&nbsp;</span></span><a href="http://www.yugangjiang.info/publication/itm_yjiang.pdf" target="_blank" style="color: rgb(38, 40, 152); text-decoration: underline; font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px;">[pdf]</a><span style="color: rgb(17, 17, 17); font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; display: inline ! important; float: none;"><span class="Apple-converted-space">&nbsp;</span></span><a shape="rect" href="javascript:togglebib('tmm10:yjiang')" class="togglebib" style="color: rgb(38, 40, 152); text-decoration: underline; font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px;">[bibtex]</a><span style="color: rgb(17, 17, 17); font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; display: inline ! important; float: none;"><span class="Apple-converted-space">&nbsp;</span></span><a href="http://vireo.cs.cityu.edu.hk/research/vireo374/" target="_blank" style="color: rgb(38, 40, 152); text-decoration: underline; font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px;">[project page]</a><span style="color: rgb(17, 17, 17); font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; display: inline ! important; float: none;"><span class="Apple-converted-space"> &nbsp;</span></span><a href="http://www.yugangjiang.info/publication/tmm10-typo.txt" target="_blank" style="color: rgb(38, 40, 152); text-decoration: underline; font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px;"></a><span style="color: rgb(17, 17, 17); font-family: tahoma; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; display: inline ! important; float: none;">IEEE Transactions on Multimedia, vol. 12, issue 1, pp. 42-53, 2010.<br>[2]
Yu-Gang Jiang, Guangnan Ye, Shih-Fu Chang, Daniel Ellis, Alexander C.
Loui, Consumer Video Understanding: A Benchmark Database and An
Evaluation of Human and Machine Performance, ACM International
Conference on Multimedia Retrieval (ICMR), Trento, Italy, Apr. 2011.<br></span></div><br></div></body></html>